## 堆外泄漏分析

### 问题描述：

JVM内存正常，Docker OOM



### Native Memory Tracking (NMT)

NMT是JVM自带的本地内存追踪工具，能够分析JVM自身分配的一些内存，但是无法追踪非JVM分配的内存

- summary：配合mode=summary使用，输出内存分配的概要信息
- detail：配合mode=detail使用，输出内存分配的概要信息+虚拟内存地址
- baseline：记录一次baseline，用于后续diff
- summary.diff：输出与baseline的概要diff，必须先执行baseline
- detail.diff：输出与baseline的详情diff，必须先执行baseline



**JDK1.8有11项输出结果，含义说明如下：**

| 名称                   | 说明                                                         | 相关JVM参数                                         |
| :--------------------- | :----------------------------------------------------------- | :-------------------------------------------------- |
| Java Heap              | Java堆占用的内存                                             | -Xmx；-Xms                                          |
| Class                  | 元空间占用的内存                                             | -XX:MetaspaceSize；-XX:MaxMetaspaceSize             |
| Thread                 | 线程占用的内存，与线程数相关                                 | -Xss                                                |
| Code                   | JIT生成的代码占用的内存                                      | -XX:InitialCodeCacheSize；-XX:ReservedCodeCacheSize |
| GC                     | 垃圾收集器使用的内存，例如G1的CardTable/RememberedSet等      |                                                     |
| Compiler               | C1/C2编译器优化代码时使用的内存                              |                                                     |
| Symbol                 | 常量池、符号表等，与String.intern也有关系                    | -XX:StringTableSize                                 |
| Native Memory Tracking | NMT使用的内存                                                |                                                     |
| Arena Chunk            |                                                              |                                                     |
| Internal               | 除了上述外的其他内存占用，一般和DirectBuffer、JVMTI有关，Unsafe_AllocateMemory |                                                     |
| Unknown                | 其他无法确定的内存                                           |                                                     |



**Reserved vs Committed vs Resident**

NMT输出的内存分为Reserved和Committed两类，使用pmap/top输出的内存有Virtual、Rss和Share几类，说明如下：

- Resident：驻留集，实际占用物理内存的大小，包括Share的内存
- Committed：占用的物理内存 + mmap标记为Read/Write的内存，近似于Resident
- Reserved：Committed + mmap标记为NONE的内存，大于Committed
- Virtual：虚拟内存空间，JDK8与Reserved近似

上述几个内存，一般看Resident和Committed，其余的没有必要关注。



**开启**

- JVM开启本地内存跟踪：-XX:NativeMemoryTracking=detail
  - 对性能有影响5%-10%

**jcmd简单查看**

- 先设置一个基准线：
  - jcmd <pid> VM.native_memory baseline
- 运行一段时间后：
  - jcmd <pid> VM.native_memory summary.diff
- 对比地看一下统计信息
  - 看是否是malloc增加



### pmap

查看内存段的情况，可多次采样分析，排除JVM的内存

```shell
pmap <pid | sort -nk2 | less

Address           Kbytes     RSS   Dirty Mode  Mapping
00007f1c9c9c3000  139016   20276       0 r--s- modules
00007f1c6aa00000   36864   23908   23908 rw---   [ anon ]
00007f1c12e00000   69632   24032   24032 rw---   [ anon ]
00007f1c3f000000   53248   25420   25420 rw---   [ anon ]
00007f1bd00fc000   98304   32776   32776 rw---   [ anon ]
00007f1be3200000   69632   36156   36156 rw---   [ anon ]
00007f1c47800000   83968   36844   36844 rw---   [ anon ]
00007f1c89700000   49152   37544   37544 rw---   [ anon ]
00007f1bd617e000   81920   49164   49164 rw---   [ anon ]
00007f1c9548b000   53568   53556   53556 rwx--   [ anon ]
00007f1bdd800000   90112   53616   53616 rw---   [ anon ]
00007f1c8df54000   60096   60040   60040 rwx--   [ anon ]
00007f1c33200000  143360   70308   70308 rw---   [ anon ]
00007f1c6fdff000  418820  417812  417812 rw---   [ anon ]
0000000500000000 12604544 8338128 8338128 rw---   [ anon ]  -- 0000000500000000就是jvm申请的内存，这里有12G
total kB         18561468 9917808 9875224
```

分析发现，堆外的内存呈现逐渐变大的趋势，初步认为就是这边内存块有问题

![image-20240118203504776](./assets/image-20240118203504776.png)

一般大多数情况是DirectByteBuffer等问题，比如Netty版本bug，可尝试在JVM中设置-Dio.netty.allocator.type=unpooled参数关闭池化，看看效果



### smaps

可以输出内存块使用的详情，包括地址范围和来源

```shell
$ cat /proc/<pid>/smaps > smaps.txt

7f1bd00bf000-7f1bd00fc000 rw-p 00000000 00:00 0 
Size:                244 kB
KernelPageSize:        4 kB
MMUPageSize:           4 kB
Rss:                 104 kB
Pss:                 104 kB
Shared_Clean:          0 kB
Shared_Dirty:          0 kB
Private_Clean:         0 kB
Private_Dirty:       104 kB
Referenced:          104 kB
Anonymous:           104 kB
LazyFree:              0 kB
AnonHugePages:         0 kB
ShmemPmdMapped:        0 kB
FilePmdMapped:         0 kB
Shared_Hugetlb:        0 kB
Private_Hugetlb:       0 kB
Swap:                  0 kB
SwapPss:               0 kB
Locked:                0 kB
THPeligible:    0
ProtectionKey:         0
VmFlags: rd wr mr mw me ac sd 
7f1bd00fc000-7f1bd60fc000 rw-p 00000000 00:00 0 
Size:              98304 kB
KernelPageSize:        4 kB
MMUPageSize:           4 kB
Rss:               32776 kB
Pss:               32776 kB
Shared_Clean:          0 kB
Shared_Dirty:          0 kB
Private_Clean:         0 kB
Private_Dirty:     32776 kB
Referenced:        32776 kB
Anonymous:         32776 kB
LazyFree:              0 kB
AnonHugePages:         0 kB
ShmemPmdMapped:        0 kB
FilePmdMapped:         0 kB
Shared_Hugetlb:        0 kB
Private_Hugetlb:       0 kB
Swap:                  0 kB
SwapPss:               0 kB
Locked:                0 kB
THPeligible:    0
ProtectionKey:         0
VmFlags: rd wr mr mw me nr sd 
```



### gdb

gdb可以dump进程的内存信息查看，知道内存中是什么内容后，可以进一步排查是否存在泄露的情况

1. 安装gdb: **sudo yum install** -y gdb
2. attach: **sudo -u adam gdb** --pid={pid}
3. dump内存: **dump memory** {path} {start_address} {end_address}（内存地址可以使用**pmap**或**less /proc/{pid}/maps**查看）
4. 查看内存中的字面量: **strings** {path}



### perf

perf主要是看线程栈调用的情况（perf record && perf report）



### 内存分配器

Jemalloc：既能提升分配效率，又能dump heap 

1. 下载Jemalloc: https://github.com/jemalloc/jemalloc/releases
2. 安装相关依赖：sudo yum install -y bzip2 gcc make graphviz

在存放Jemalloc压缩包的目录下执行如下操作，假设jemalloc的版本为{version}

1. 解压：**tar** **-xvf** jemalloc-{version}.tar.bz2
2. 编译：**cd** jemalloc-{version} && **./configure** --enable-prof && **make** && **sudo make install**
3. 验证：**jeprof** --version

使用

安装好Jemalloc后，需要在JVM启动时指定使用Jemalloc进行内存分配

指定方式为在JVM参数中设置如下环境变量：

```shell
export LD_PRELOAD=/usr/local/lib/libjemalloc.so
export MALLOC_CONF=prof:true,lg_prof_interval:30,lg_prof_sample:17,prof_prefix:/mnt/mesos/sandbox/{name}/jeprof
#
#LD_PRELOAD: 指定了在JVM启动时需要链接的文件。
#MALLOC_CONF: 指定了Heap profiling的参数。
# - prof：是否需要做Heap profiling。
# - lg_prof_interval：Heap profiling的间隔，以2为底数的指数，单位为bytes，30意味着每分配（2^30 bytes = 1 GB）就生成一dump文件。
# - lg_prof_sample：Heap allocation采样的精度，以2为底的指数，单位bytes，17意味着每分配（2^17 bytes = 128 KB）就做一次采样，值越高保真度越低，但程序性能越好。
# - prof_prefix：Heap dump的路径及前缀，需要确保程序对该路径有写入权限，生成的文件名格式为<prefix>.<pid>.<seq>.i<iseq>.heap。
```

生成dump报告:

```shell
# 如果上述步骤进行无误，程序运行后一段时间可以在**{prof_prefix}**指定的目录下看到一些名为jeprof.*.heap的文件。执行如下命令：
sudo -u adam /usr/local/bin/jeprof --show_bytes --pdf /usr/java/jdk1.8/bin/java /mnt/mesos/sandbox/{name}/jeprof.* > ~/output.pdf
# 执行完成后就可以看到Heap dump的内容了，通过该内容可以判断是哪里产生了Native Memory分配
```





####  其他：

- linux glibc内存泄漏：https://heapdump.cn/article/1709425
- jemalloc
  - https://github.com/jemalloc/jemalloc/wiki/Getting-Started
  - http://jemalloc.net/jemalloc.3.html#opt.lg_prof_sample
  - https://github.com/jeffgriffith/native-jvm-leaks
  - [Troubleshooting_native_memory_leaks.pdf](http://conf.ctripcorp.com/download/attachments/477283528/Troubleshooting_native_memory_leaks_1540301908390001k6DR.pdf?version=1&modificationDate=1608097838000&api=v2)